# One-shot Learning Networks

Another project we worked on this summer was create [one-shot learning] networks. One-shot learning means that the network is fed a much smaller amount of training data to see how well it can learn from smaller chunks of data which is more realistic. The accuracies achieved using one-shot learning are usually pretty sporadic and tend to level off much lower. Furthermore, one-shot networks tend to be overfit. I wrote three one-shot networks for each data set that I was working with in order to compare the accuracies to the [LSH one-shot accuracies]. The toy problem network uses a validation set and goes back and chooses the best supports to determine the best support accuracy.

ADD MORE INFO ABOUT ONE-SHOT LEARNING

## Network

TO DO

## Data Sets

TO DO

## Use

TO DO

## Results

TO DO -- also link to google sheets and add graphs (maybe even link to the LSH page)

## Future Work 

TO DO

[one-shot learning]: https://en.wikipedia.org/wiki/One-shot_learning
[LSH one-shot accuracies]: https://github.com/slancas1/budapest_research/tree/master/LSH
